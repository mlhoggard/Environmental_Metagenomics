{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oxford Nanopore data processing: prokaryote isolate sequencing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data prep and QC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "- [1.1 General notes](#1.1-General-notes)\n",
    "- [1.2 Data prep](#1.2-Data-prep)\n",
    "- [1.3 QC](#1.3-Read-QC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 General notes\n",
    "\n",
    "The examples below are based on sequencing of **12 isolates**, multiplexed during ONT library prep (each tagged with a unique barcode). Long-read sequencing data was generated on an Oxford Nanopore GridION, with basecalling based on high accuracy basecalling (HAC; Q9). In this example, basecalling was done in real time during sequencing. However, you can also opt to perform basecalling separately on the raw data after the fact. In our tests, comparable (generally identical) steps were also appropriate for processing data based on super accuracy basecalling model (SUP; Q10). \n",
    "\n",
    "Note: \n",
    "\n",
    "- Using HAC data can increase coverage (compared with SUP data), but at the expense of a lower quality threshold for individual reads. Coverage depth can be an important factor in the completeness and accuracy of genome assemblies downstream, and so can be important to consider when choosing between HAC and SUP data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate read chunks for each barcode\n",
    "\n",
    "Basecalling data for all barcodes are output from a minION/gridION in chucks. The chunks for all *used* barcodes first need to be concatenated into single files for each barcode. If samples were run split into replicates across multiple barcodes, these can also be concatenated together.\n",
    "\n",
    "NOTE: \n",
    "\n",
    "- There can be a small number of reads assigned to unused barcodes. This represents a low level error rate in the process of assigning reads correctly (cross-talk). It pays to be conscious of the fact that there will likely be a small fraction of incorrect reads in each of your sample data sets, although these should be easy to spot downstream based on differential coverage between the correct sequences and those from cross-talk. For this step, we can simply ignore reads in the unused barcodes' directories.\n",
    "- If you have run replicate samples across multiple barcodes (to increase data generated), and/or if you have the same samples run over multiple sequencing runs, you can choose whether to pool these here or instead process each separately and generate duplicate assemblies downstream. In this example, we will assume samples were run in duplicate on a single sequencing run, and we wish to pool these prior to assembly and downstream work.\n",
    "\n",
    "This example assumes the raw data are contained within `/working/dir/0.ONT_data_HAC/*/fastq_pass/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory\n",
    "cd /working/dir\n",
    "mkdir -p 1.ONT_data_HAC_concatenated/0.concat_barcodes\n",
    "\n",
    "# Loop through each barcode , and for each, concatenate all chunks (fastq.gz files) into one fastq.gz file\n",
    "for barcode_path in 0.ONT_data_HAC/*/fastq_pass/barcode*; do\n",
    "    barcode=$(basename ${barcode_path})\n",
    "    cat 0.ONT_data_HAC/*/fastq_pass/${barcode}/*.fastq.gz > 1.ONT_data_HAC_concatenated/0.concat_barcodes/${barcode}.fastq.gz\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Concatenate isolate replicates and rename as sequential isolate IDs\n",
    "\n",
    "Concatenate replicates together for each isolate (if split across multipe barcodes during sequencing). You may also wish to rename isolates here for ease of downstream use (for example, the steps that follow assume that all samples are named based on sequential isolateIDs (e.g. isolate_1, isolate_2, etc.).\n",
    "\n",
    "A simple (incomplete) example to achieve this via `cat` commands is given below. (Although, for a large number of samples this may be cleaner to achieve via a loop incorporating arrays of barcode IDs and sample numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory\n",
    "cd /working/dir/1.ONT_data_HAC_concatenated/\n",
    "mkdir -p 1.concat_replicates\n",
    "\n",
    "# Concatenate replicates of same samples, and rename as isolate_n\n",
    "cat 0.concat_barcodes/barcode01.fastq.gz 0.concat_barcodes/barcode06.fastq.gz > 1.concat_replicates/isolate_1.fastq.gz\n",
    "cat 0.concat_barcodes/barcode02.fastq.gz 0.concat_barcodes/barcode07.fastq.gz > 1.concat_replicates/isolate_2.fastq.gz\n",
    "cat 0.concat_barcodes/barcode03.fastq.gz 0.concat_barcodes/barcode08.fastq.gz > 1.concat_replicates/isolate_3.fastq.gz\n",
    "#... etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3 Read QC \n",
    "\n",
    "There are several options to assess read quality statistics. In this example we will generate basic read quality stats via *NanoStat*.\n",
    "\n",
    "NOTE: \n",
    "\n",
    "- It may take a few mins per sample to run this. If connection drop outs are an issue, you could run this remotely via, e.g., *slurm* or *tmux*\n",
    "- If you get very fragmented assemblies downstream, you opt to apply a filter here (e.g. length or quality filter) to remove some of the poorer reads, which *may* improve assemblies downstream. Some options for this include *nanofilt* or *filtlong*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run *Nanostat* on data for all 12 isolates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory\n",
    "cd /working/dir\n",
    "mkdir -p 1.ONT_data_HAC_concatenated/1.concat_replicates/NanoStat\n",
    "\n",
    "# Load modules\n",
    "module purge\n",
    "module load NanoStat/1.5.0-gimkl-2020a-Python-3.8.2\n",
    "\n",
    "# Run NanoStat on each barcode dataset\n",
    "for i in {1..12}; do\n",
    "    NanoStat -t 8 --tsv \\\n",
    "    -n 1.ONT_data_HAC_concatenated/1.concat_replicates/NanoStat/isolate_${i}_NanoStat.tsv \\\n",
    "    --fastq 1.ONT_data_HAC_concatenated/1.concat_replicates/isolate_${i}.fastq.gz\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Nano stat results into one summary table \n",
    "\n",
    "NOTE:\n",
    "\n",
    "- This will ultimately be put into a script for ease of use. But for now we can use the python code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory\n",
    "cd /working/dir\n",
    "\n",
    "# Load python\n",
    "module purge\n",
    "module load Python/3.8.2-gimkl-2020a\n",
    "python3\n",
    "\n",
    "### Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Compile NanoStat results\n",
    "results_list = []\n",
    "for i in range (1,13):\n",
    "    tmp_df = pd.read_csv('1.ONT_data_HAC_concatenated/1.concat_replicates/NanoStat/isolate_'+str(i)+'_NanoStat.tsv', sep='\\t')\n",
    "    tmp_df.index=['isolate_'+str(i)]*len(tmp_df)\n",
    "    tmp_df = tmp_df.pivot(columns='Metrics', values='dataset')\n",
    "    results_list.append(tmp_df)\n",
    "\n",
    "# Generate summary table and write out\n",
    "results_df = pd.concat(results_list, axis=0)\n",
    "results_df.index.name = 'isolateID'\n",
    "results_df.to_csv('1.ONT_data_HAC_concatenated/1.concat_replicates/NanoStat/summary_table_NanoStat.tsv', sep='\\t')\n",
    "\n",
    "quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
